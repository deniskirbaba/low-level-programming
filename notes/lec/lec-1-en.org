#+INCLUDE: "common/org-header.org" 
#+LANGUAGE: en
#+TITLE: Chapter 1. Introduction to the computer systems architecture 

# <
* Introduction to the computer systems architecture 
# >
  
  # [
    
  In this course we are studying programming languages, mostly low-level, in the context of complex computer systems. We will introduce the notion of systems, computer systems and we will explain why is the systemic approach important for programmers and computer engineers alike.

  Note that this is not a course in systems theory so a lot of concepts can be expanded or made more precise.

  There are two main sources of information for this course:
    
  - The systemic aspect is well described in the book "Principles of Computer System Design: an Introduction". It is used in MIT to teach a two-semester course in computer systems.
  - The programming aspect is described in the book "Low-level programming; C, Assembly and Program Execution".

    
  # ]
  -----

* References
  :PROPERTIES: 
  :UNNUMBERED: t
  :END:
  
    # [
    In every part I will provide references to external sources. You can pass the course without studying them, but I encourage you to read them because they extend the course and place it in a broader context.

    For today's part the references are as follows:
    
    # ]
    
  - "Low-level programming": Chapter 1
  - "Principles of Computer System Design: an Introduction":
    - Chapter 1 "Systems" p. 2--41.
    - Chapter 2, "ยง2.1 The Three Fundamental Abstractions" p. 45--59.

  -----

* Systems

# <
  - *System*  is a set of interconnected components that together have properties that the individual parts do not. 

** Examples
   - Car :: spare parts can not transport wares, but a car can.
   - Human :: individual cells are not sentient, a human is.
   - Computer :: CPU is unable to compute unless powered up and plugged into a computer. 
# >
     # [

     In this course we are interested in a particular kind of computer systems. To provide context, let us start by giving a precise definition of a system in general, not necessary a computer system.

     What is a system? The most generic definition would be: a collection of interacting components.

     The defining characteristic of systems is that interactions between their components result in new properties of the system as a whole; we call such properties /systemic/ or /emergent/. 

     For example, a car has a systemic property: it can carry people and objects around. None of its part can transport you like the car does. Moreover, when the car is fully assembled but its engine is shut down, the car stays in place and is unable to move. That is because only the interaction between the parts of a car allows it to move, providing us with a useful functionality.

     A living *human* is different from a dead corpse. Our cells are interacting to enable us to walk, think, breathe and, generally, making us alive and sentient. These are our systemic properties.

     A *computer* can not perform any work unless powered up and turned on. The ability to perform computations are its systemic property, born from interactions of software and CPU, memory, storage, buses and other hardware components.
         
     People often say: "a system is more than a sum of its parts". Indeed, a system is not only a sum of its parts, but also interactions between them.
    
     # ]
   
   -----

** Environment
# <
   - Systems work in /environment/
     - A car interacts with the driver and with objects around it
# >
       # [
       Every system exists in an /environment/. For example, a car interacts with the driver and road, the trees around it, other cars. Humans interact with other humans, with sounds and visuals, with the air that we breathe and the food that we eat. A computer interacts with other devices, with human through screens, keyboards and other interfaces and so on.
       # ]

# <
   - A system can be a part of a bigger system.
     - Car computer is a subsystem of car but it has many subsystems itself. 
       # >
       # [
       Generally speaking, an environment consists of everything of interest placed around the system: noises, sources of information, objects the system may be interacting with, other systems, or a parent super-system, of which our system of interest is a part. The environment usually includes an organized part and a chaotic part.

       Note that the environment only includes such objects that can potentially interact with the system. It it not useful to include in the car's environment blue whales, sound waves, bacteria or asteroids.

       # ]

     # <
   - Every system is connected to its environment through its /interface/ 
     - Wheels, knobs, buttons...
      # >
       # [
       Interactions with systems happen through their /interface/. For a car its controls, as well as its wheels are parts of interface. For us, humans, our eyes, ears, skin are parts of our interface.
       The interface can vary depending on the system's function: if we consider a verbal conversation between two humans, their interface is a spoken word; however, when these humans are dancing together, the interfacing happens through visual contact and touch.

       The notion of interface is closely related to the border between system and its environment. The borders of a system are usually blurred. We can postulate that the border consists of such parts of the system that can be attributed to either the system itself or its environment.
       # ]


   -----

** Computer systems
   # <
   In computer systems we are observing inner processes through the interface and interpret the observations. These are computations.

# >
   # [

   We understand what systems are now. But what are computer systems? We are not studying cars or human biology after all.

   Here is what makes computer systems distinct --- or not so much. We can peek through their interfaces and observe the processes inside; then we /interpret/ our observations. And this is what constitutes /the computations/ --- our interpretation of what we observe in systems.

   Why did I tell that the computer systems are /not so much distinct/? Because our interpretation is a crucial part of the process. To demonstrate this I will provide some examples of such systems where computations are happening, but not in electric circuits.

   # ]
*** Computing in nature

    - random.org :: Getting random numbers from atmospheric noise.

      # [
      It is a common knowledge that computers are not good with random numbers. Unless they incorporate a specific hardware component to generate randomness, they are unable to achieve true non-determinism, only imitate it through clever mathematical trickery. But random numbers are extremely useful, especially in cryptography.

      There is a service that you can access at random.org. It provides good quality random numbers by using electromagnetic noise from the atmosphere as a seed for the random number generator. Using such numbers in cryptography is much more sane than those generated by a traditional computer.

      We can say that the part of random.org that performs computations is the atmosphere of Earth itself. This may seem quite unorthodox because it comes from the nature, it is not man-made. However observing atmospheric noise and interpreting our observations, then plugging it into a more conventional computer system provides us with numbers with very particular properties. What is it, if not a computing process?

      # ]
    - Pulsars :: High-precision clock.
      # [
      Another example are pulsars. Pulsars are stars at late stages of their life, which are emitting bursts of light with high frequency. They are doing it in a very stable way and thus can be used as clock: we count the bursts and know how many milliseconds have passed. Counting time is also computation.
      # ]

    - Ant colony :: optimizes paths from the colony to food.
    # [
    The third example are ant colonies. Ants are searching for food in a process that can be roughly described as a random walk. As they move, they mark their steps with a scent of pheromones.

    When an ant worker reaches the food or other interesting resource it goes back to the ant colony in exactly the same way, tracing back its steps. Thanks to the smelly footprints it can go back following exactly the same route that led this little ant to food.
    
    However its path can be far from optimal: it may contain loops, or it may walk uphill instead of going around. In order to optimize the path the ants use their capacities to recognize stronger and weaker smells and thus get a sense of where the route is going, and which routes are more frequently stepped used by more ants. This knowledge seems enough to be able to shorten the paths substantially.

    So when you put a piece of food in the proximity of ant colony, and when they find it, by observing ants you may get a solution to a problem of finding path from ant colony to the food.

    This is a computation that does not look any different from those performed by computers, and actually ant colonies have inspired some path finding algorithms.

    To sum it up, computations can be performed in a lot of ways, and they are closely tied up with interpretation. Sometimes nature performs computations without computers, and we are saying so because we are interpreting natural processes as computations.
    # ]
    
    -----

** Programs as systems
  # < 
   - A running program is a part of computer system
# >
    # [

    Our main interest in this course is programming. What is the relationship between programming and computer systems?

    When we program, the outcome of our work is a program, more precisely, its source code. It may be executed through interpretation, it may be compiled and then executed in a different form --- it does not matter.
    # ]
    # <
   - Source code is not a system.
    # > 
    # [
    What matters is that the source code is not a system. Systems are alive, their parts are interacting and new properties emerge from these interactions. Source code is dead. It is like a manual on how to assemble a piece of furniture, addressed to someone who is going to perform the actual work.

    What is our interest in systems then? When we launch program and it is being executed, it becomes a part of a hybrid system incorporating hardware and software parts. It is impossible to understand its behavior without thinking about hardware, and software, and their interaction.

    # ]


    #+ATTR_HTML: :border 0 :frame none :rules none
    | Source code | Progarm |
    |-----------------------------+---------------------------|
    | airplane blueprint | flying airplane |
    | furniture assembly manual | working furniture assembler|
    
    
    # [
    If we draw an analogy between, on one hand, the source code and the program being executed, and something else, then the source code relates to the running program as a manual on how to assemble a piece of furniture relates to a system of furniture assembler, instruction manual and furniture parts; this system is performing work on assembling the furniture.

    # ]

    -----
** System decomposition
    # <
    At least two ways of decomposing a system:
   - Structurally :: what are the parts?
# >
     
      # [
      Now we want to elaborate how to identify the parts that constitute a system. Given a system, possibly very complex, how to decompose it in parts? It is not an easy question to answer: there are several ways on decomposing a system.

      The first, and most obvious way, is to decompose the system spatially. When a system is built from parts like a house is built of bricks, we are talking about its structural decomposition; the system is a whole, and the smaller pieces are its parts.
      # ]
      
# <
   - Functional :: what are the roles?
# >
      # [
      The second, and more useful way, is to decompose the system functionally. What does each part of system do? This is not the same as structurally decomposing system, because we only care about the functionality, the roles of parts of the system.
      # ]
      
*** Example: scissors 

      # [
      Take scissors as an example of a system. It is able to cut paper better than a simple knife because of its systemic properties.
      # ]
     # < 
    - three structural: two pieces of metal and a screw.
# >

      # [
      Structurally scissors are composed of two pieces of metal and a screw that connects them together. 
      # ]
  # <    
    - two functional parts: handle and cutter
    # >  
      # [
      Functionally, the same scissors consist of two parts: one is used to hold them, whereas another cuts paper. Not only do scissors have more structural parts than functional, we can not map structural parts to functional parts.

      In a way, functional parts are /roles/ that people play. Consider a system consisting of people working on the same project. Structurally, every person is a part of such system. Functionally, the parts of this system are /roles/: programmers, designers, managers, testers etc. One of the differences between them is that one person can play several roles (e.g. be a programmer and project manager, or programmer and graphics designer).

      Functional way of viewing systems is considered much more efficient for engineering computer systems including programs. It emphasizes the important aspect --- functionality.
      # ]
    -----

*** Pointer
   # < 
   [Mouse] pointer is a functional component. 

   Mouse, tablet --- or a program controlling pointer.
   # > 
    # [
    To get accustomed to structural and functional decompositions let us study some examples: this time, from computer systems and programming.

    The first example is a cursor; we all see it constantly on computer screen as we point at things, drag them or otherwise interact with them.
    
    Cursor is a functional component, not a structural one. It is not equivalent to mouse, graphical tablet or any other device: a program can take control over the cursor and move it around.
      
    # ]
    -----

*** Virtual memory
# <
    - Are there addresses not mapped to anything?
   # >   
      # [

      A lot of programs are being executed simultaneously on your computer. They coexist and are isolated from one another through virtual memory.

      Consider virtual memory. Is it a structural part of computer system or a functional one?

      The function of virtual memory is to store data. The virtual memory space itself consists of areas of forbidden addresses, files, mapped from hard drive, anonymous pages and so on. The virtual memory is backed by the physical memory, storage (in case of memory swapping or mapping files) and operating system, which ensures transparent operation with it.
      
      It does not seem possible to divide computer system into structural parts in a way that isolates one virtual memory space from all others, so it is not a structural component.
      # ]
    -----
*** Process (operating systems)
# <
    - Are we able to spatially decompose a system so that one process would be separated from all others?
# >
      # [
      Another example is a process --- we have all seen a list of processes in a task manager. A similar argument is applicable to it: we can not divide computer on parts structurally to isolate one process. A process contains a virtual address space too, parts of which are shared among processes. So a process is also a functional component.
      # ]
    -----
    
*** Storage
# <
    - ramfs;
    - distributed file systems. 
  # >  
      # [
      The final example is a storage  --- its name alone suggests its function and hence that it is a functional component. It can be implemented as a piece of hardware, but the data can be stored in a distributed filesystem, in a distributed database, in cloud. In cloud storage it is not clear where a specific piece of data is stored due to factors like replication blurring it. Generally speaking, storage is a functional component.
      # ]
    -----
    
**  Complexity of systems


      # [
      Now we know what computer systems are, we have studied how to decompose them into parts in a structural or functional way; we have provided some examples of their functional components.
      
      Modern computer systems have a lot of components both structurally and functionally; it suggests that they are /complex/. But what is the complexity we are talking about here? And what is complexity at all?

      # ]
      Complexity is the ultimate challenge when building systems.
      # [
      There are different approaches to defining complexity, and some work better depending on the system.
      # ]
      
      
   1. How difficult is to describe a system? Sometimes measured in bits.

      # [
      In the first approach we consider a system to be complex when it is difficult to describe. As an example, imagine we describe the system through programming and we have fixed a programming language to do that. Then a measure called Kolmogorov complexity shows the minimal length of a computer program that can possibly describe the system.
      # ]
      
       Keywords: Information, entropy, algorithmic complexity, Fisher's
      information, Renyi's entropy, code length (Hamming, Shennon-Fano),
      Chernov's information, Lempel-Ziv complexity, Kolmogorov complexity.
       
   2. How hard is to craft a system?

      # [
      In the second approach we postulate that a system is complex when it is difficult to create. Creation can be measured in energy or money, but also a working computer system requires performing computations. Systems that require more computations, or more computational resources like memory for temporary data, are then deemed complex.

      A well known way of describing complexity in this manner is using algorithmic, data or communication complexity, usually in asymptotic form. Drawing a parallel with Kolmogorov complexity, there is a measure called logical depth that corresponds to the fastest computer program able to  describe the system.
      # ]
      
      Keywords: Algorithmic space/time complexity, logical depth, termodynamic depth, crypticity.

   3. How regular is the system?

      # [
      In the third approach the system is complex if it is irregular. As an
      example, imagine a map-reduce engine. Such an engine takes a huge problem
      and a huge chunk of data. Then it splits the problem into smaller
      independent subproblems, each using a portion of data. The subproblems are
      distributed over a number of computational nodes, then the engine
      aggregates their solutions into the final answer to the huge problem.

      Is this system complex? It may seem huge, and we can have thousands of
      computing nodes, but all these nodes are the same; they fit the same
      description. Being able to describe multiple parts of the system in the
      same way decreases its complexity. Think about it when you plan your
      programs, because reusing code usually makes them less complex.
      # ]
     
      Keywords: entropy.
      
   -----

** Signs of complexity

      # [
      While computing an exact metric of complexity is usually impossible, there are some signs that suggest that the system is complex according to one of these metrics.
      # ]
      
   1. Many nodes.
   2. Many connections.
   3. Many irregularities
   4. Huge description.
   5. Huge team working on a system.


   -----
* Fighting complexity

      # [

#+begin_quote
Debugging is twice as hard as writing the code in the first place. Therefore, if you write the code as cleverly as possible, you are, by definition, not smart enough to debug it. -- Kernighan's law.
#+end_quote


      So, for practical reasons we need to build computer systems that solve complex problems. But such systems tend to be complex themselves. It makes them harder to build, maintain, and develop. Without a right approach we may not even be able to complete the construction of the system because modifying it will become unbearably difficult. For instance, if all parts are tightly interdependent, modifying one part may produce a ripple effect that would force us to modify all parts that depend on it.
      # ]

      We have discovered approaches to fighting complexity empirically:
    
  - Modularity ::  build system parts in isolation, then combine them. 

      # [
      Modularity implies that we dissect the system on smaller parts that are easier to digest; then we repeat the dissection until we face simple challenges.
      We have to be able to develop modules in isolation, without touching other modules.
      Then we can ascend the hierarchy, building more complex blocks with the modules that we already have.
      # ]
    
  - Abstraction :: forget the module structure, only describe the behavior.
      # [
      Abstraction goes together with modularity and makes it even more useful. If modularity is just about dissection of system into independent parts, abstraction is about forgetting and simplifying the description of these parts.
For example, when two programs communicate through Internet, we are
usually not thinking about how electrical signals pass through wires, and that
the packets may get corrupted and we have to correct errors in them. This is
achieved by abstracting the exchanging mechanism so that we get an illusion that getting information from the network is as simple as accessing a local file.

      # ]

  -----
** Composition
# <
Linking the modules together: names.

Interactions outside interfaces.

# >
   # [
Once we are done crafting modules in isolation we start putting them together.
In order to organize interfacing between modules we need to provide names for the points of interaction. A good naming scheme may be hard to create.

Furthermore modules may start to interact outside the interfaces e.g. through their common state: global variables, files etc.
This worsens the issue of error propagation when errors appear in one module and break the functioning of other modules.
To prevent it we carefully limit these interactions architecturally by separating state e.g. in a client-service architecture.
We will encounter the issues related to the module composition frequently.
   # ]

   #+begin_comment
TODO Expand
   #+end_comment
  -----
** Three types of functional components
# [

Earlier we have provided examples of computer systems where the computation is performed in an unorthodox way, such as ant colonies. It suggests that in reality we can make computer systems out of virtually anything, and only our creativity is the limit here. However, this is not a very useful statement because we need some guidelines to start exploring the vast chaotic space of possible computer systems.

As diverse as computer systems can be, once we pick a typical computer system
apart and perform its functional decomposition we will see that its components
are usually falling into one of three categories.
   # ]
   
        - Memory :: storing data.
        - Interpreter :: reaction on events, executing actions.
        - Communication link :: connects parts of system.

        # [
   Before we explain why do we care about these components, let us provide some examples for each of them.
   # ]
   -----
*** Examples of memory
# [
Variants of memory may be the easiest to recall. Memory can be implemented in a
variety of ways, some of which are mostly hardware, others are mixed
hardware-software solutions.

Here are some examples of memory in computer systems:

# ]
    - Triggers
    - Registers
    - RAM
    - Cache
    - Virtual memory
    - HDD/SSD
    - RAID
    - Databases
    - Filesystems
    - Cloud storage

    -----

*** Examples of interpreters

# [

Interpreters are active or reactive components of computer systems.

Here are some examples of interpreters:
# ]

    - CPU
    - Controllers
    - Interpreters and compilers
    - Virtual machines
    - Word
    - Browsers
    - Games

    -----

*** Examples of transport

# [
Finally, communication link connects other components allowing them to interact. Multiple components can be connected; in this case transport has to perform some kind of routing to transport data from sender to a correct receiver.

Here are some examples of communication links:
# ]
    - Different types of cables
    - Wi-Fi
    - USB
    - Internet
    - Telegram
    - Email
    - Sockets
    - Pipes
    - Buffers

    -----
* Computer systems constructor

# [
We care about these three types of functional components. Why? To give you a
taste, no matter how memory is implemented, be it a database or a hard disk
drive, there are a number of universal memory characteristics such as atomicity
of reads and writes, or latency, or coherence between reads and writes. These
characteristics pose certain challenges, like decreasing memory latency, and
there are ways of dealing with such problems which are independent from the
specific memory implementation.

These components also provide us with a useful language to describe computer systems.
We can represent a system with a needed level of detail as a combination of memories and interpreters connected by transports.
Remember, though, that this is a /functional decomposition/, not structural
blocks. There are parts of systems that store data, not hardware modules!

# ]

  Let us study the different combinations of three fundamental types of functional components:

** von Neumann architecture

#+INCLUDE: "common/before-image.org"
#+attr_latex: :width 0.4\textwidth
#+attr_html: :width 40%
{{{if-latex-else([[./img/von-neumann-pure.svg]],)}}}
#+INCLUDE: "common/after-image.org"

#+BEGIN_EXPORT md
<p align='center'> <img width='40%'  src='./img/von-neumann-pure.svg' /> </p>
#+END_EXPORT
   
   
    
# [
If we try to keep the architecture as simple as possible, we will have a combination of exactly  one interpreter and one memory.
This architecture is known as /von Neumann architecture/.
In this architecture memory and CPU (that acts as an /interpreter/) have properties that seem trivial to us: for example, instructions and data are binary encoded. In other architectures such properties will not necessary hold.
# ]
   -----

** Network between two computers

   # [
Now imagine two programs that exchange data via network. We can depict this system as follows:
# ]

#+INCLUDE: "common/before-image.org"
#+attr_latex: :width 0.40\textwidth
#+attr_html: :width 40%
{{{if-latex-else([[./img/network.svg]],)}}}
#+INCLUDE: "common/after-image.org"

#+BEGIN_EXPORT md
<p align='center'> <img width='60%'  src='./img/network.svg' /> </p>
#+END_EXPORT

# [
The line connecting two memories is a transport enabling us to send and receive data as if we were copying data between two memories. A lot of details are abstracted away by this transport: the work of operating system, device drivers, network adapter, intermediate nodes, all the protocols from Ethernet, ARP to TCP/IP, resolving IP addresses, finding a route from one computer to another and so on. These details may not be important for us so on a diagram we just draw a straight line between two memories.
# ]

# <
Transport can be very complicated and hide OS, packet routing, path finding, re-sending lost packets etc.
   # >
   -----

# [
Now let us see another configuration consisting of two interpreters and one memory. What can it depict? Some options are:
# ]
   - multiprocessor system with shared memory;
   - multiple threads in a process sharing the same memory;
   - interpreter of a programming language written in a different programming language.

   #+INCLUDE: "common/before-image.org"
   #+attr_latex: :width 0.40\textwidth
   #+attr_html: :width 40%
   {{{if-latex-else([[./img/compiler.svg]],)}}}
   #+INCLUDE: "common/after-image.org"

   #+BEGIN_EXPORT md
   <p align='center'> <img width='60%'  src='./img/compiler.svg' /> </p>
   #+END_EXPORT


# [
It is interesting to think about such schemes and ask questions. The connected components on such schemes can interact, depending on the components the nature of interaction will be different.

Studying all possible types of interactions may reveal potential problems that should be aware of while crafting such systems.

For example, if two threads are sharing the same memory and issue the writes to the same location, which write will be applied first? What if the write is big enough so that it is not atomic, and both writings will be happening at the same time? Can it so happen that the multibyte value written to memory will be a mixture of bytes from two different values coming from different threads?

# ]
   -----

    
* Computer system of interest

# [
Now we will give an overview of the computer system that we will study: Intel 64/AMD64 architecture running programs that are written in a higher-level language like C.
The C language is a low-level language which is close enough to the hardware, so it is not too hard to relate the programs in C to the machine code that is actually executed.

This computer system is structured in layers, as shown by the following schematic:

#+INCLUDE: "common/before-image.org"
#+attr_latex: :width 0.90\textwidth
#+attr_html: :width 90%
{{{if-latex-else([[./img/overview.svg]],)}}}
#+INCLUDE: "common/after-image.org"

We do not have to understand all the levels in detail, but let us give a quick overview of what is happening there.

- 


# ]


# <
#+BEGIN_EXPORT md
<p align='center'> <img src='./img/overview.svg' /> </p>
#+END_EXPORT
# >



# [
We are going to grow it from a seed which is a von Neumann architecture with one processor and one memory.

# ]

** von Neumann architecture

# [
Decades ago early computers mimicked von Neumann architecture with their hardware blocks,
but they have soon grown to be much more sophisticated.
However, many industrial programming languages such as Java, C or JavaScript give us an impression that the programs in these languages perceive the computer in the same way.
This is not a coincidence, and is closely related to a notion of /model of computations/, which we will elaborate in our course a bit later.

The von Neumann architecture is where we start to grow our model until we reach a sufficient approximation of how a low-level programmer perceives the computer.
# ]



#+INCLUDE: "common/before-image.org"
#+attr_latex: :width 0.40\textwidth
#+attr_html: :width 40%
{{{if-latex-else([[./img/von-neumann-pure.svg]],)}}}
#+INCLUDE: "common/after-image.org"

#+BEGIN_EXPORT md
<p align='center'> <img width='%'  src='./img/von-neumann-pure.svg' /> </p>
#+END_EXPORT
    
-----
** Interrupts
# < 
   Lack of interactivity.
# >


#+INCLUDE: "common/before-image.org"
#+attr_latex: :width 0.40\textwidth
#+attr_html: :width 40%
{{{if-latex-else([[./img/von-neumann-int.svg]],)}}}
#+INCLUDE: "common/after-image.org"

# [
First, we notice that von Neumann architecture has no connection to the outside world. It is implied that the programmer has somehow an access to memory to change its contents, but the system can not react to the events of outside world.


Interrupts allow the computer to change program execution order based on events external to the program itself. After a signal is caught, which can be external or internal, the program execution is suspended, some registers are saved and CPU starts executing a special routine to handle the situation.

Interrupts occur in many situations, for example:
  - A signal from an external device.
  - Zero division.
  - Invalid instruction (when CPU failed to recognize an instruction by its binary representation).
  - An attempt of executing a privileged instruction in a non-privileged mode.

# ]
   -----


** von Neumann with registers

#+INCLUDE: "common/before-image.org"
#+attr_latex: :width 0.50\textwidth
#+attr_html: :width 50%
{{{if-latex-else([[./img/von-neumann-regs.svg]],)}}}
#+INCLUDE: "common/after-image.org"

#+BEGIN_EXPORT md
<p align='center'> <img width='60%'  src='./img/von-neumann-regs.svg' /> </p>
#+END_EXPORT

# <
- Works thanks to locality:
   - temporal locality.
   - spatial locality.
# >
# [
The data exchange between CPU and memory is a crucial part of computations in a von Neumann computer. Instructions have to be fetched from memory, operands have to be fetched from memory; some instructions also store results in memory.

This creates a bottleneck and leads to wasted CPU time when it waits for the data response from the memory chip. To avoid constant wait, a processor was equipped with its own memory cells, called registers. These are few but fast.
Programs are usually written in such a way, that most of the time the working set of memory cells is small enough. This fact suggests that programs can be written so, that most of the time CPU will be working with registers. 

Registers are based on transistors, while main memory uses condensers. We could have implemented main memory on transistors and get a much faster circuit.

There are several reasons for which engineers prefer other ways of speeding up computations.

    - Registers are less cheap.
    - Instructions encode register's number as parts of their codes. To
        address more registers the instructions have to grow in size. 
    - Registers add complexity to the circuits to address them. More
        complex circuits are harder to speed up. It is not easy to set up
        a large register file to work on 5 GHz.

Naturally, register usage slows down computer in the worst case. If everything
has to be fetched into registers before the computations are made and flushed
into memory afterwards, what's the profit? 

*** Locality
After years of programming people have discovered a property of programs, which happens to be true, an empirical property. 
It is a result of how programs are typically written, not a law of nature.
 It is called locality of reference and there are two main types of it: /temporal/ and /spatial/.

- Temporal locality means, that accesses to one address are likely to be close in time. 
- Spatial locality means, that after accessing an address $X$ the next memory access will likely to be close to $X$, like $X-16$ or $X+28$.

These properties are not binary: you can write a program exhibiting stronger or
weaker locality. 

Typical programs are using the following pattern: the data working set is small and can be kept inside registers. After fetching the data into registers once we will work with them for quite some time, then the results will be flushed into the memory. The data stored in memory will be rarely used by the program. In case we need to work with this data we will loose performance because:

- We need to fetch data into the registers.
- If all registers hold data that we still need later on, we will have to spill some of them, i.e. save their contents into temporally allocated memory cells.  


        
# ]
-----

** More complex, more effective
# <
   - Increasing performance in average, killing the worst case.
# >
   # [
Registers provide us a useful insight. They demonstrate a very common situation in engineering: we decrease performance in the worst case to improve it in average. This is a good technique unless we are building real time systems, which impose constraints on the worst system reaction time. Think about a system that controls a power plant. It has to react to all events in a very short time and if it lags and misses the event, the whole system fails.

Real time systems have to always respect a fixed delay between an event and a reaction to it, so decreasing performance in the worst case is unacceptable.

These tricks make systems more complex to use some resource more efficiently. In this case, the resource is CPU. This is also a universal rule.

# ]
-----

# <
   Attempts on using a resource more effectively usually makes the system more complex.
     # >
# [
Imagine a single-track railroad line running through a long, narrow
canyon. To improve the utilization of the single track we may allow trains to run both ways at the same time by installing a switch and a short side track in a wide spot about halfway through the canyon.
Then through careful scheduling we may allow trains pass each other by putting one of them at a side track. However, the train operations are now much more complex than they used to be. If either train is delayed, the schedules of both are disrupted. We also need a control system to prevent possible human mistakes in operating trains.
And the system has now a new systemic property: the trains have a limit on their length. If two trains are to pass in the middle, at least one of them must be short enough to pull completely onto the side track.

   # ]
   [[./img/train.svg]]
-----

** Memory levels
  # <
  Memory is slow. Registers, cash, slow storage.
# >

   
   # [
   Next step is putting several layers of cache memory between registers and memory. We follow the same principle of putting intermediate memory layers between CPU and memory.
   Cache is controlled by CPU and is invisible to a programmer; however, we may observe its effects on memory access time. If our programs respect certain rules, for example, favor consecutive memory accesses over random ones, cache is going to speed up their execution. Otherwise we will experience cache misses which result in a lower latency when accessing memory.
   # ]
#+INCLUDE: "common/before-image.org"
   #+attr_latex: :width 0.90\textwidth
   #+attr_html: :width 90%
   {{{if-latex-else([[./img/von-neumann-complete.svg]],)}}}
   #+INCLUDE: "common/after-image.org"

   #+BEGIN_EXPORT md
   <p align='center'> <img width='90%'  src='./img/von-neumann-complete.svg' /> </p>
   #+END_EXPORT

   # [
   We also connect a huge and slow layer of memory called /storage/. It serves as an archive of data, a large library of data units. In our computers storage is usually a solid state drive (SSD) or a hard disk drive (HDD).
   # ]
   
   -----


** Stack

# <
   - Isolate function local variables.
   - Call and return.
     
   Hardware stack, as a part of main memory.

# >

# [

Our programs contain a number of functions calling each other. It is useful to teach our computers to support three features of functions:

- Functions can be called from anywhere.
- When the function terminates it has to return to the place where it was called.
- Functions have local variables; it is the data that belongs to the function alone.

Memory is a flat, linear address space, where every byte has an address.
Such memory is not a good fit to implement these features right away.
This is why we first emulate a /hardware stack/ on top of main linear memory. 

Stack in general is a data structure supporting two operations: pushing an element on top of it and popping the topmost element.
Hardware stack implements this abstraction on top of memory through special instructions and a register =rsp= called /stack pointer/.
It stores an address of the topmost stack element.
A stack is used not only in computations but to store local variables and support functions call sequence in programming languages.

There is no special stack memory in our computers; it means that we can bypass the abstraction of stack and access its elements by their memory addresses.

There is a hardware support for such data structure.
It does not mean there is also a separate stack memory.
It is just sort of an emulation implemented with two machine instructions (=push= and =pop=) and a register =rsp=. The instructions perform as follows:

- =push argument=
   - The =rsp= value is decreased by 8. 
   - An argument is stored in memory starting at the address, taken from the modified =rsp=.
- =pop argument=
   - The topmost stack element is copied into the register/memory, depending on the argument;
   - The =rsp= register is increased by the size of its argument.

   #+begin_comment
   todo: images; maybe details are irrelevant?
   #+end_comment
# ]

   -----
** Virtualization, multitasking


# [
Then we throw in the multitasking support where the CPU and memory get virtualized, giving every program an illusion that it is the only one using CPU and memory.
This isolates programs and prevents error propagation between them by separating their address spaces.
It also prevents program from executing dangerous privileged instructions which only the operating system and device drivers should be able to use.
Instructions like =inb=, used to communicate with devices, or assignment to the system registers like =cr3=, which is a part of the virtual memory mechanism, are privileged.
# ]
# <
  - Virtualize CPU
  - Isolate processes
  - restrict privileged instructions, only OS allowed
# >

#+INCLUDE: "common/before-image.org"
#+attr_latex: :width \textwidth
#+attr_html: :width 100%
{{{if-latex-else([[./img/final-approximation.svg]],)}}}
#+INCLUDE: "common/after-image.org"

#+BEGIN_EXPORT md
<p align='center'> <img width='90%'  src='./img/final-approximation.svg' /> </p>
#+END_EXPORT

#+begin_comment
Need to redraw this diagrams.
#+end_comment
   -----
** Loading programs
 # <
Compiler generates meta-information.

Loader uses it to create and initialize a correct virtual address space.
 # >
  # [
 Compiled programs are usually stored as files in the storage.
 Before we are able to execute a program we have to:

   

* Role of programming languages

TODO

- Higher levels of computer system
- Need to be able to express more complex ideas of computations and then execute
- Different languages are better for different purposes --- parts of system described in different languages then composed together.
- Compose programs from the language constructions, compile, link together, then load and execute on virtualized hardware.

# [
# ]


   -----
 1) create an address space for it;
 2) load the instructions and data from the stored executable file;
 3) initialize all associated structures (like standard input, output and error streams, signal handlers etc.)

    A special part of the operating system called /loader/ is responsible for that.
    
    Creating address space and loading the program is tied to the program instructions themselves.
    Instructions contain addresses of functions to be called and variables to be read.
    So in some cases it is important to load programs at specific addresses.
   Because of this the compiler is usually producing  a structured file where instructions and data are marked with meta-information describing what kind of routines there are and where should they be  loaded.
    The loader then relies on this meta-information to create and initialize the address space.
    We will study one such structured format, ELF.
  # ]

  -----  
 -----
  # < 
* Conclusion
  In the next part: more on interpreters, model of computations, examples.

  - programming is not limited to: while/if, function calls, objects, classes 
  - assembly program structure
  - how to craft the most difficult parts of the first assignment easily 
 # >

 #+begin_comment
 To add/modify:

 - composition using names (addresses, symbols etc)
 - layering and overview of the big schematic in the end

 #+end_comment
